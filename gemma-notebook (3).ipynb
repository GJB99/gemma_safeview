{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":450403,"databundleVersionId":12845828,"modelInstanceId":365533}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SightSafe üëÅÔ∏è\n### Offline Visual Narrator powered by Gemma 3n\nHelping people with low-vision understand their surroundings ‚Äì privately and without an internet connection.\nThis notebook shows a working proof-of-concept for SightSafe, an on-device assistive app that:\nCaptures an image from the user (camera or gallery).\nGenerates a rich scene description with Gemma 3n.\nLets the user ask follow-up questions about the image.\nRuns entirely offline ‚Äì perfect for real-world mobility and privacy.\nThe same logic can be wrapped in a mobile UI (Android/iOS) or deployed on a Raspberry Pi / Jetson for wearable glasses.\nKey Gemma 3n capabilities used\n‚Ä¢ Multimodal : interleaved image + text understanding.\n‚Ä¢ On-device ready : the 4B ‚Üí 2B sub-model keeps RAM < 4 GB.\n‚Ä¢ Mix‚Äôn‚ÄôMatch sub-models: dynamically switch between fast 2B and high-quality 4B.\nWhy this matters ‚Äì 285 million people worldwide live with visual impairment. Offline, private narration removes data-plan costs, latency, and privacy concerns often associated with cloud vision APIs.","metadata":{}},{"cell_type":"markdown","source":"Install & set-up\nWe use the bleeding-edge Transformers >= 4.54 (Gemma 3n support) and optionally Unsloth for memory-efficient fine-tuning.\nRunning inside Kaggle/Colab? Skip CUDA drivers ‚Äì they are pre-installed.\n\n1st time running: unncomment and run the following 2 cells once, then restart and comment the 2 cells again.","metadata":{}},{"cell_type":"code","source":"#!pip uninstall -y torch torchvision torchaudio numpy scipy scikit-learn tensorflow keras","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%pip install -qU \\\n#  numpy==1.26.4 \\\n#  torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \\\n#  fsspec==2025.3.0 gcsfs==2025.3.0 \"rich<14\" \\\n#  transformers accelerate timm unsloth kagglehub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"]   = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T11:25:32.245182Z","iopub.execute_input":"2025-07-02T11:25:32.245571Z","iopub.status.idle":"2025-07-02T11:25:32.255153Z","shell.execute_reply.started":"2025-07-02T11:25:32.245543Z","shell.execute_reply":"2025-07-02T11:25:32.253701Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Download Gemma 3n\nGoogle hosts Gemma 3n weights on Kaggle Hub. Change the flavour (e.g. gemma-3n-e2b-it) to switch model sizes.","metadata":{}},{"cell_type":"code","source":"import kagglehub, os, gc, torch\nfrom pathlib import Path\nMODEL_REPO = \"google/gemma-3n/transformers/gemma-3n-e2b-it\"  # 4B base with 2B sub-model\nMODEL_DIR = Path(kagglehub.model_download(MODEL_REPO))\nprint(\"Model files stored in:\", MODEL_DIR)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T11:25:39.327052Z","iopub.execute_input":"2025-07-02T11:25:39.329331Z","iopub.status.idle":"2025-07-02T11:25:48.120009Z","shell.execute_reply.started":"2025-07-02T11:25:39.329275Z","shell.execute_reply":"2025-07-02T11:25:48.117141Z"}},"outputs":[{"name":"stdout","text":"Model files stored in: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Quick image ‚Üí description demo\nWe load the multimodal Gemma 3n checkpoint and send an image with a prompt asking for a detailed, concise scene description.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForImageTextToText\n\nprocessor = AutoProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)\nmodel = AutoModelForImageTextToText.from_pretrained(\n        MODEL_DIR, torch_dtype=\"auto\", device_map=\"auto\")\n# ==== finished loading model === \n\n#reference image\nIMAGE_URL = \"https://images.pexels.com/photos/845451/pexels-photo-845451.jpeg?h=512\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": IMAGE_URL},\n            {\"type\": \"text\",  \"text\": \"Describe this scene for someone who cannot see it.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,          # <-- let the processor handle tokenization\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device, dtype=model.dtype)\n\ngen = model.generate(**inputs, max_new_tokens=128)\ndescription = processor.batch_decode(gen[:, inputs[\"input_ids\"].shape[-1]:],\n                                     skip_special_tokens=True)[0]\nprint(description)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T11:30:07.987476Z","iopub.execute_input":"2025-07-02T11:30:07.987886Z","iopub.status.idle":"2025-07-02T11:54:41.895869Z","shell.execute_reply.started":"2025-07-02T11:30:07.987851Z","shell.execute_reply":"2025-07-02T11:54:41.894645Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d9cab730d8477abf832942327f56ec"}},"metadata":{}},{"name":"stdout","text":"The scene is a bright, modern office space, captured from a slightly elevated angle looking down at a person working at a desk. The dominant feature is a large window that stretches almost the entire height of the frame on the left side, flooding the room with natural light. The light is intense, suggesting it might be daytime.\n\nBelow the window, a long wooden desk is visible, running from the top of the frame towards the viewer. It's cluttered with various office items. On the desk, there are multiple computer monitors, keyboards, and other peripherals ‚Äì it looks like a shared workspace or a densely occupied area. \n\nA\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Follow-up Q&A about the same image\nGemma 3n keeps the image in context (no re-upload needed). Ask clarifying questions ‚Äì \"Is it safe to cross the street?\", \"How many people do you see?\", etc.","metadata":{}},{"cell_type":"code","source":"# First user message + image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": IMAGE_URL},\n            {\"type\": \"text\",  \"text\": \"Describe this scene for someone who cannot see it.\"}\n        ]\n    }\n]\n\ndef run(messages, max_tokens=128):\n    inputs = processor.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    ).to(model.device, dtype=model.dtype)\n\n    gen   = model.generate(**inputs, max_new_tokens=max_tokens)\n    reply = processor.batch_decode(\n                gen[:, inputs[\"input_ids\"].shape[-1]:],\n                skip_special_tokens=True\n            )[0].strip()\n    return reply\n\n# 1Ô∏è‚É£ assistant answers\nanswer = run(messages)\nprint(\"Assistant:\", answer)\nmessages.append(\n    {\"role\": \"assistant\",\n     \"content\": [{\"type\": \"text\", \"text\": answer}]}\n)\n\n# 2Ô∏è‚É£ user follow-up question\nquestion = \"Tell me different details?\"\nmessages.append(\n    {\"role\": \"user\",\n     \"content\": [{\"type\": \"text\", \"text\": question}]}\n)\n\n# assistant reply to follow-up\nanswer2 = run(messages, max_tokens=64)\nprint(\"\\nAssistant:\", answer2)\nmessages.append(\n    {\"role\": \"assistant\",\n     \"content\": [{\"type\": \"text\", \"text\": answer2}]}\n)\n\n# ‚Ä¶continue alternating as needed","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:00:30.657777Z","iopub.execute_input":"2025-07-02T12:00:30.659371Z","iopub.status.idle":"2025-07-02T12:48:13.826146Z","shell.execute_reply.started":"2025-07-02T12:00:30.659307Z","shell.execute_reply":"2025-07-02T12:48:13.824952Z"}},"outputs":[{"name":"stdout","text":"Assistant: The scene is viewed from a high angle, looking down at a modern office workspace. The dominant feature is a man sitting at a desk, working on a computer. He's wearing headphones and a light-colored long-sleeved shirt. His posture suggests he's focused on the screen. \n\nThe desk is made of a light-colored wood and has several computer components: a monitor, a keyboard, and a mouse. Wires are visible connecting these devices. To the left of the man, another desk is partially visible, also with a monitor and other equipment. \n\nThe office is bright, likely due to large\n\nAssistant: Okay, let's delve into more details of this office scene, describing it in a more comprehensive way:\n\nThe photograph captures a contemporary office environment from a slightly elevated perspective, looking down onto a row of workstations. The main subject is a man diligently working at his desk. He appears to be in his late\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"5. (Optional) Fine-tune with Unsloth LoRA\nFor high accuracy on assistive narration, you can LoRA-tune Gemma 3n on datasets like VizWiz Captions.\nHere we show a tiny 20-sample demo to keep runtime < 2 min. Remove the slice to train on full data.","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom datasets import load_dataset\nsft_model, sft_tokenizer = FastLanguageModel.from_pretrained(MODEL_DIR, max_seq_length=4096)\ndataset = load_dataset('MBZUAI/VizWiz_Captions', split='train[:20]')\ndef format_example(e):\n    return { 'text': f'User: [IMAGE]\\nDescribe this image.\\nAssistant: {e[\"caption\"]}' }\ndataset = dataset.map(format_example, remove_columns=dataset.column_names)\nFastLanguageModel.for_inference(sft_model)  # enable gradient checkpointing etc.\nsft_model = FastLanguageModel.get_peft_model(\n        sft_model,\n        r=8, lora_alpha=16, lora_dropout=0.05, target_modules='all')\nsft_model.train()\nsft_model.fit(dataset, batch_size=2, epochs=1, lr=1e-4)\nsft_model.save_pretrained('sightsafe-lora')\ngc.collect(); torch.cuda.empty_cache()","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:12:37.231501Z","iopub.execute_input":"2025-07-02T13:12:37.232045Z","iopub.status.idle":"2025-07-02T13:12:37.299016Z","shell.execute_reply.started":"2025-07-02T13:12:37.232011Z","shell.execute_reply":"2025-07-02T13:12:37.297189Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_235/3791614154.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  from unsloth import FastLanguageModel\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_235/3791614154.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msft_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MBZUAI/VizWiz_Captions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train[:20]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformat_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Reduce VRAM usage by reducing fragmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."],"ename":"NotImplementedError","evalue":"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"6. Lightweight Gradio demo (runs offline)\nUpload a photo, click Describe, and optionally ask follow-up questions. The interface swaps between 2B and 4B modes on-the-fly depending on device RAM.","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\ndef describe_and_chat(image, question, use_fast):\n    chosen_submodel = '2B' if use_fast else '4B'\n    # The mix'n'match trick would load weights slices; here we just inform the UI.\n    msgs = [{ 'role':'user', 'content':[{'type':'image','image':image}, {'type':'text','text':question or 'Describe this scene in detail.'}] }]\n    inp = processor.apply_chat_template(msgs, add_generation_prompt=True, return_tensors='pt').to(model.device)\n    gen = model.generate(**inp, max_new_tokens=128)\n    return processor.decode(gen[0], skip_special_tokens=True), f'Processed with the {chosen_submodel} sub-model.'\n\nwith gr.Blocks() as app:\n    gr.Markdown('# SightSafe ‚Äì Offline Visual Narrator')\n    image = gr.Image(type='filepath', label='Upload photo')\n    question = gr.Textbox(label='Optional follow-up question', placeholder='What colour is the bus?')\n    use_fast = gr.Checkbox(label='Fast 2B mode (less VRAM, quicker)')\n    btn = gr.Button('Describe')\n    out_text = gr.Textbox(label='Assistant')\n    meta = gr.Markdown()\n    btn.click(describe_and_chat, [image, question, use_fast], [out_text, meta])\n\napp.launch(debug=False, share=False)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T16:00:31.621307Z","iopub.execute_input":"2025-07-02T16:00:31.622270Z","iopub.status.idle":"2025-07-02T16:00:31.721081Z","shell.execute_reply.started":"2025-07-02T16:00:31.622236Z","shell.execute_reply":"2025-07-02T16:00:31.719750Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4244244237.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdescribe_and_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchosen_submodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2B'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_fast\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'4B'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# The mix'n'match trick would load weights slices; here we just inform the UI.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"],"ename":"ModuleNotFoundError","evalue":"No module named 'gradio'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"Next Steps\nPackage the notebook into an Android APK using Google AI Edge SDK.\nAdd text-to-speech with PyTTSx3 for auditory output.\nExpand Q&A with long-term scene memory using RecurrentGemma techniques.","metadata":{}}]}